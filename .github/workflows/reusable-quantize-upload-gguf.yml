# This workflow:
# - Downloads the f16 (quantized) GGUF model from the target (model) 'repo_id' defined in the build matrix.
#   - Note: The f16 model was generated and uploaded by the `XX-hf-convert-to-f16-gguf` workflow
#     whose completion triggers this workflow.
# - Quantizes the f16 GGUF to the designated 'quantization' (e.g., Q4_0, Q4_K_M, etc.) defined in the build matrix.
# - Uploads the quantized GGUF to target 'repo_id'.

name: quantize-upload-gguf-reusable

on:
  workflow_call:
    secrets:
      hf_token:
        required: true
    inputs:
      enable_language_jobs:
        type: boolean
        required: false
        default: false
      enable_guardian_jobs:
        type: boolean
        required: false
        default: false
      enable_embedding_jobs:
        type: boolean
        required: false
        default: false
      enable_docling_jobs:
        type: boolean
        required: false
        default: false
      repo_id:
        type: string
        required: true
      quantization:
        type: string
        required: true
      hf_collection_config:
        type: string
        required: true
      target_repo_owner:
        type: string
        required: true
      target_repo_name_ext:
        type: string
        required: true
      debug:
        type: boolean
        required: true
        default: false
      trace:
        type: boolean
        required: false
        default: false

env:
  DEF_SEP: "-"
  DEF_PRECISION: "f16" # "bf16"
  EXT_GGUF: .gguf
  CONVERTED_FILE_EXISTS: "0"
  UPLOAD_CONVERTED_ENABLED: true
  FIND_ARGS: "-not -path '*/.*' -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models

jobs:
  quantize:
    runs-on: macos-latest
    if: ${{ inputs.enable_language_jobs == true || inputs.enable_guardian_jobs == true || inputs.enable_embedding_jobs == true || inputs.enable_docling_jobs == true }}
    env:
      HF_HUB_DISABLE_XET: 1
      HF_HUB_ENABLE_HF_TRANSFER: 0
      LLAMA_CPP_VERSION: b6808 # b6791 # "b6569" # 'b5717', # 'b6050'
    steps:
    - uses: actions/checkout@v4

    - run: |
        echo "[INFO] github.env:"
        echo "[INFO] >> run_id: '${{ github.run_id }}'"
        echo "[INFO] >> ref: '${{ github.ref }}', ref_name: '${{ github.ref_name }}', ref_type: '${{ github.ref_type }}'"
        echo "[INFO] >> workflow_ref: '${{ github.workflow_ref }}'"
        echo "[INFO] >> run_id: '${{ github.run_id }}'"
        echo "[INFO] >> event_type: '${{ github.event_type }}'"
        echo "[INFO] >> event.: action: '${{ github.event.action }}'"
        echo "[INFO] >> event.: base_ref: '${{ github.event.base_ref }}'"
        echo "[INFO] >> event.: workflow_run.conclusion: '${{ github.event.workflow_run.conclusion }}'"
        echo "[INFO] >> event.release.: name: '${{ github.event.release.name }}', tag_name: '${{ github.event.release.tag_name }}'"

    - name: confirm-environment
      run: |
        uname -a
        echo "runner.os: ${{ runner.os }}"
        echo "runner.arch: ${{ runner.arch }}"
        echo "github.workspace: ${{ github.workspace }}"
        echo "env.UPLOAD_CONVERTED_ENABLED: ${{ env.UPLOAD_CONVERTED_ENABLED }}"
        echo "env.FIND_ARGS: ${{ env.FIND_ARGS }}"
        echo "env.LLAMACPP_DIR: ${{ env.LLAMACPP_DIR }}"
        echo "env.MODEL_DOWNLOAD_DIR: ${{ env.MODEL_DOWNLOAD_DIR }}"
        echo "env.DEF_SEP: ${{ env.DEF_SEP }}"
        echo "env.DEF_PRECISION: ${{ env.DEF_PRECISION }}"
        echo "env.EXT_GGUF: ${{ env.EXT_GGUF }}"

    # - name: Dump runner context
    #   env:
    #     RUNNER_CONTEXT: ${{ toJson(runner) }}
    #   run: echo "$RUNNER_CONTEXT"

    # - name: Dump GitHub context
    #   env:
    #     GH_CONTEXT: ${{ toJson(github) }}
    #   run: echo "$GH_CONTEXT"

    # - name: Dump GitHub quiet
    #   env:
    #     GH_QUIET: ${{ toJson(secrets) }}
    #   run: echo "$GH_QUIET"

    - name: Dump GitHub inputs
      env:
        GITHUB_INPUTS: ${{ toJson(inputs) }}
      run: echo "$GITHUB_INPUTS"

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    # Note: you can verify in a step using: run: python -c "import sys; print(sys.version)"
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: install-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        pip install -U huggingface_hub
        pip list

    # Use this step to set values to the github context (shared across jobs/steps)
    # Note: using $GITHUB_OUTPUT sets values under the current step's namespace
    # whereas using $GITHUB_ENV sets values in the job's underlying environment.
    # Note: for each 'repo_id' we parse out e.g., REPO_ORG=ibm-granite REPO_NAME=granite-3.0-2b-instruct
    - name: set-github-env
      id: set_github_env
      run: |
        echo "REPO_ORG=$(dirname '${{ inputs.repo_id }}')" >> $GITHUB_ENV
        echo "REPO_NAME=$(basename '${{ inputs.repo_id }}')" >> $GITHUB_ENV
        echo "EXT_PRECISION_GGUF=${{ env.DEF_SEP }}${{env.DEF_PRECISION}}${{env.EXT_GGUF}}" >> $GITHUB_ENV

    - name: set-derivative-env-vars-1
      run: |
        echo "BASE_FNAME_CONVERTED_GGUF=${{ env.REPO_NAME }}${{env.EXT_PRECISION_GGUF}}" >> $GITHUB_ENV
        echo "BASE_FNAME_QUANTIZED_GGUF=${{ env.REPO_NAME }}-${{inputs.quantization}}${{env.EXT_GGUF}}" >> $GITHUB_ENV
        echo "LOCAL_MODEL_PATH=${{env.MODEL_DOWNLOAD_DIR}}/${{ inputs.repo_id }}" >> $GITHUB_ENV

    # NOTE: we MUST have the target quant. model (output) in a sep. directory as the source f16 model (input)
    # or streaming will cause a SIGSEV error
    - name: set-derivative-env-vars-2
      run: |
        echo "LOCAL_FNAME_CONVERTED_GGUF=${{env.MODEL_DOWNLOAD_DIR}}/${{ inputs.repo_id }}/$BASE_FNAME_CONVERTED_GGUF" >> $GITHUB_ENV
        echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.BASE_FNAME_QUANTIZED_GGUF}}" >> $GITHUB_ENV
        echo "TARGET_REPO_ID=${{inputs.target_repo_owner}}/${{env.REPO_NAME}}${{inputs.target_repo_name_ext}}" >> $GITHUB_ENV

    - name: verify-github-env
      run: |
        echo "================== GITHUB_ENV ====================="
        echo "REPO_ORG=$REPO_ORG"
        echo "REPO_NAME=$REPO_NAME"
        echo "EXT_PRECISION_GGUF=$EXT_PRECISION_GGUF"
        echo "================== Derivative Environment Variables 1 =================="
        echo "BASE_FNAME_CONVERTED_GGUF='$BASE_FNAME_CONVERTED_GGUF' (${{ env.BASE_FNAME_CONVERTED_GGUF }})"
        echo "BASE_FNAME_QUANTIZED_GGUF='$BASE_FNAME_QUANTIZED_GGUF' (${{ env.BASE_FNAME_QUANTIZED_GGUF }})"
        echo "LOCAL_MODEL_PATH='$LOCAL_MODEL_PATH' (${{ env.LOCAL_MODEL_PATH }})"
        echo "================== Derivative Environment Variables 2 =================="
        echo "LOCAL_FNAME_CONVERTED_GGUF='$LOCAL_FNAME_CONVERTED_GGUF' (${{ env.LOCAL_FNAME_CONVERTED_GGUF }})"
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"
        echo "TARGET_REPO_ID='$TARGET_REPO_ID' (${{ env.TARGET_REPO_ID }})"

    - name: test-gguf-converted-file-exists-in-hf
      if: env.UPLOAD_CONVERTED_ENABLED == 'true'
      run: |
        output=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_CONVERTED_GGUF }} ${{secrets.hf_token}})
        echo "output: '$output'"
        if [[ "$output" == "True" ]]; then
          echo "CONVERTED_FILE_EXISTS=1" >> $GITHUB_ENV
        elif [[ "$output" == "False" ]]; then
          echo "CONVERTED_FILE_EXISTS=0" >> $GITHUB_ENV
        else
          echo "Unexpected output from Python script: $output"
          exit 2
        fi

    - name: download-converted-gguf-using-hf-download
      if: env.CONVERTED_FILE_EXISTS == '1'
      run: |
        echo "Downloading f16 GGUF using hf_file_download..."
        echo "Repo ID: ${{ env.TARGET_REPO_ID }}, Model Name: ${{ env.BASE_FNAME_CONVERTED_GGUF }}"
        echo "--------------------"
        python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_CONVERTED_GGUF }} ${{secrets.hf_token}}
        ls -al ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/*.gguf
        echo "LOCAL_FNAME_CONVERTED_GGUF=${{ env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/${{ env.BASE_FNAME_CONVERTED_GGUF }}" >> $GITHUB_ENV

    # - name: download-converted-gguf-curl
    #   if: env.CONVERTED_FILE_EXISTS == '1'
    #   run: |
    #     echo "Downloading using curl..."
    #     echo "--------------------"
    #     curl -C - -f --create-dirs -H "Authorization: Bearer ${{secrets.hf_token}}" -o ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/${{ env.BASE_FNAME_CONVERTED_GGUF }} -L https://huggingface.co/${{ env.TARGET_REPO_ID }}/resolve/main/${{ env.BASE_FNAME_CONVERTED_GGUF }}
    #     chmod -R +rwx ${{env.MODEL_DOWNLOAD_DIR}}
    #     echo "--------------------"
    #     echo "LOCAL_FNAME_CONVERTED_GGUF=${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/${{ env.BASE_FNAME_CONVERTED_GGUF }}" >> $GITHUB_ENV

    # - name: download-converted-gguf-curl
    #   if: env.CONVERTED_FILE_EXISTS == '1'
    #   run: |
    #     echo "CONVERTED_FILE_EXISTS='$CONVERTED_FILE_EXISTS' (${{ env.CONVERTED_FILE_EXISTS }})"
    #     echo "--------------------"
    #     mkdir -p ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}
    #     chmod -R +rw ${{env.MODEL_DOWNLOAD_DIR}}
    #     find ${{env.MODEL_DOWNLOAD_DIR}} ${{env.FIND_ARGS}}
    #     echo "--------------------"
    #     ./llama.cpp/scripts/hf.sh --repo ${{ env.TARGET_REPO_ID }} --file ${{ env.BASE_FNAME_CONVERTED_GGUF }} --outdir ${{ env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}
    #     echo "--------------------"
    #     echo "LOCAL_FNAME_CONVERTED_GGUF=${{ env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/${{ env.BASE_FNAME_CONVERTED_GGUF }}" >> $GITHUB_ENV

    # for GGUF conversion, we only need a shallow copy (depth=1) and
    # specify only the scripts and requirements files we need (sparse).
    # Note: we include LICENSE for future BOM generation
    - name: shallow-clone-llamacpp
      uses: actions/checkout@v4
      with:
        path: ${{ env.LLAMACPP_DIR }} # checkout under this directory
        repository: 'ggerganov/llama.cpp'
        ref: ${{env.LLAMA_CPP_VERSION}} # default to 'master' branch
        fetch-depth: 1
        sparse-checkout-cone-mode: false
        sparse-checkout: |
          ./LICENSE
          convert_hf_to_gguf.py
          convert_lora_to_gguf.py
          requirements/requirements-convert_legacy_llama.txt
          requirements/requirements-convert_hf_to_gguf.txt
          requirements/requirements-convert_lora_to_gguf.txt
          gguf-py
          scripts

    - name: print-llamacpp
      if: inputs.debug == true
      run: |
        echo "PATH: '{{$PATH}}'"
        ls -al llama.cpp
        find llama.cpp ${{env.FIND_ARGS}}

    - name: install-llama-cpp-dependencies
      run: |
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_hf_to_gguf.txt
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_lora_to_gguf.txt
        pip uninstall --yes transformers
        pip install transformers==4.49.0
        pip list

    - name: download-model-snapshot
      if: env.CONVERTED_FILE_EXISTS == '0'
      run: |
        python ./scripts/hf_model_download_snapshot.py ${{env.MODEL_DOWNLOAD_DIR}} $REPO_ORG $REPO_NAME
        find models ${{env.FIND_ARGS}}

    - name: convert-hf-to-gguf
      if: env.CONVERTED_FILE_EXISTS == '0'
      run: |
        python ./llama.cpp/convert_hf_to_gguf.py ${{ env.LOCAL_MODEL_PATH }} --outfile ${{ env.LOCAL_FNAME_CONVERTED_GGUF }} --verbose
        ls -al ${{ env.LOCAL_MODEL_PATH }}/*.gguf

    - name: print-storage-memory-info
      if: inputs.debug == true
      run: |
        df
        echo "--------------------"
        xcodebuild -version
        clang --version
        echo "--------------------"
        top -l 1 | grep -E "^CPU|^PhysMem"
        echo "--------------------"

    - name: verify-downloaded-files
      if: inputs.debug == true
      run: |
        echo downloaded files...
        echo "--------------------"
        ls -al -d */
        echo "--------------------"
        find . -name \*.gguf -type f
        echo "--------------------"

    - name: quantize-gguf
      run: |
        echo "--------------------"
        echo "LOCAL_FNAME_CONVERTED_GGUF='$LOCAL_FNAME_CONVERTED_GGUF' (${{ env.LOCAL_FNAME_CONVERTED_GGUF }})"
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"
        echo "--------------------"
        sudo ./bin/llama-quantize ${{ env.LOCAL_FNAME_CONVERTED_GGUF }} ${{ env.LOCAL_FNAME_QUANTIZED_GGUF }} ${{inputs.quantization}}

    - name: find-gguf-files
      if: inputs.debug == true
      run: |
        find . -name \*.gguf -type f
        ls -al .
        echo "--------------------"

    - name: delete-previous-upload
      if: env.UPLOAD_CONVERTED_ENABLED == 'true'
      run: |
        exists=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_REPO_ID }} ${{ env.LOCAL_FNAME_QUANTIZED_GGUF }} ${{secrets.hf_token}})
        echo "exists: '$exists'"
        if [[ "$exists" == "True" ]]; then
          python ./scripts/hf_file_delete.py ${{ env.TARGET_REPO_ID }} ${{ env.LOCAL_FNAME_QUANTIZED_GGUF }} ${{secrets.hf_token}}
        fi

    - name: upload-model-gguf
      run: |
        echo "TARGET_REPO_ID=${{env.TARGET_REPO_ID}}"
        echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.LOCAL_FNAME_QUANTIZED_GGUF}}"
        python ./scripts/hf_model_upload.py ${{ env.TARGET_REPO_ID }} ${{ env.LOCAL_FNAME_QUANTIZED_GGUF }} ${{secrets.hf_token}} ${{github.workflow_ref}} ${{github.run_id}}
