# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python package

#on:
#  release:
#    types: [created]
#  workflow_dispatch:

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models

jobs:
  build:

    # runs-on: ubuntu-latest
    runs-on: macos-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11"]

    steps:
    - uses: actions/checkout@v4
    - name: confirm-os-arch
      run: |
        uname -a
        echo "Operating System: ${{ runner.os }}"
        echo "Architecture: ${{ runner.arch }}"
        echo "python-version: ${{ matrix.python-version }}"

    # - name: alias-tree
    #   if: runner.os == 'macOS'
    #   run: |
    #     alias tree="find . -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"
    #     tree
    
    # - name: apt-get tree
    #   if: ${{ contains(runner.os, 'ubuntu') }}      
    #   run: |
    #     sudo apt-get update
    #     sudo apt-get install -y tree        

    # - name: install-linux-commands
    #   run: |
    #     sudo apt-get update
    #     sudo apt-get install -y tree

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11' 

    # verify/log version
    - name: display-python-version
      run: python -c "import sys; print(sys.version)"      

    # for GGUF conversion, we only need a shallow copy (depth=1) and
    # specify only the scripts and requirements files we need (sparse).
    # Note: we include LICENSE for future BOM generation
    - name: shallow-clone-llamacpp
      uses: actions/checkout@v4
      with:
        path: ${{ env.LLAMACPP_DIR }} # checkout under this directory
        repository: 'ggerganov/llama.cpp'
        ref: 'master'  # default to master branch
        fetch-depth: 1
        sparse-checkout-cone-mode: false
        sparse-checkout: |
          LICENSE
          convert_hf_to_gguf.py
          convert_lora_to_gguf.py
          requirements/requirements-convert_legacy_llama.txt
          requirements/requirements-convert_hf_to_gguf.txt
          requirements/requirements-convert_lora_to_gguf.txt
          gguf-py
          scripts

    - name: print-llamacpp
      run: |
        echo "PATH: '{{$PATH}}'"
        ls -al llama.cpp
 
    - name: install-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_hf_to_gguf.txt
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_lora_to_gguf.txt
        pip list

    - name: download-model
      run: |
        python ./scripts/download_model_from_hf.py ${{env.MODEL_DOWNLOAD_DIR}} ibm-granite granite-3.0-8b-instruct

    - name: convert-hf-to-gguf
      run: |
        pwd
        find models -type f -name '*.safetensors'
        ls -al ./llama.cpp/convert*
        python ./llama.cpp/convert_hf_to_gguf.py models/ibm-granite/granite-3.0-8b-instruct/ --outfile test.gguf --verbose
        find models -not -path '*/.*' | sort

    - name: quantize-gguf
        run: |
          ./bin/llama-quantize test.gguf

    # TODO:
    # - name: convert-lora-to-gguf
    #   run: |
    #     python ./llamacpp/convert_lora_to_gguf.py \
