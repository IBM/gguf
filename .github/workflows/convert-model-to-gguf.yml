# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python package

#on:
#  release:
#    types: [created]
#  workflow_dispatch:

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models
  FIND_ARGS: "-not -path '*/.*' -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"

jobs:
  build:
    # runs-on: ubuntu-latest
    runs-on: macos-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11"]
        repo_id: ["ibm-granite/granite-3.0-8b-instruct"]

    steps:
    - uses: actions/checkout@v4
    - name: confirm-os-arch
      run: |
        uname -a
        echo "Operating System: ${{ runner.os }}"
        echo "Architecture: ${{ runner.arch }}"
        echo "python-version: ${{ matrix.python-version }}"
        echo "repo_id: ${{ matrix.repo_id }}"        

    # alias tree="find . -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"    
    # To allow `alias` in the default, non-interactive shell override using `shopt`.
    - name: create-aliases
      if: runner.os == 'macOS'
      run: |
        shopt -s expand_aliases
        alias tree="find"

    # - name: install-linux-commands
    #   run: |
    #     sudo apt-get update
    #     sudo apt-get install -y tree

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Uncomment step to verify/capture python version in bash:    
    # - name: display-python-version
    #   run: python -c "import sys; print(sys.version)"      

    - name: set-github-env
      id: github_env
      run: |
        echo "full_path=models/ibm-granite/foo" >> $GITHUB_OUTPUT

    - name: verify-github-env
      run: |   
        echo "full_path=${{ steps.github_env.outputs.full_path }}"

    # for GGUF conversion, we only need a shallow copy (depth=1) and
    # specify only the scripts and requirements files we need (sparse).
    # Note: we include LICENSE for future BOM generation
    - name: shallow-clone-llamacpp
      uses: actions/checkout@v4
      with:
        path: ${{ env.LLAMACPP_DIR }} # checkout under this directory
        repository: 'ggerganov/llama.cpp'
        ref: 'master'  # default to master branch
        fetch-depth: 1
        sparse-checkout-cone-mode: false
        sparse-checkout: |
          LICENSE
          convert_hf_to_gguf.py
          convert_lora_to_gguf.py
          requirements/requirements-convert_legacy_llama.txt
          requirements/requirements-convert_hf_to_gguf.txt
          requirements/requirements-convert_lora_to_gguf.txt
          gguf-py
          scripts

    - name: print-llamacpp
      run: |
        echo "PATH: '{{$PATH}}'"
        ls -al llama.cpp
        find llama.cpp ${{env.FIND_ARGS}}
 
    - name: install-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_hf_to_gguf.txt
        python -m pip install -r ./llama.cpp/requirements/requirements-convert_lora_to_gguf.txt
        pip list

    - name: download-model
      run: |
        pwd
        echo "MODEL_DOWNLOAD_DIR: ${{env.MODEL_DOWNLOAD_DIR}}"
        echo "download_dir='$MODEL_DOWNLOAD_DIR'"        
        python ./scripts/download_model_from_hf.py $MODEL_DOWNLOAD_DIR ibm-granite granite-3.0-8b-instruct
        find models ${{env.FIND_ARGS}}

    - name: convert-hf-to-gguf
      run: |
        echo "MODEL_DOWNLOAD_DIR: ${{env.MODEL_DOWNLOAD_DIR}}"      
        echo "download_dir='$MODEL_DOWNLOAD_DIR/ibm-granite/granite-3.0-8b-instruct/'"
        ls -al models
        python ./llama.cpp/convert_hf_to_gguf.py $MODEL_DOWNLOAD_DIR/ibm-granite/granite-3.0-8b-instruct/ --outfile $MODEL_DOWNLOAD_DIR/ibm-granite/granite-3.0-8b-instruct/f16.gguf --verbose

    - name: quantize-gguf
      run: |
        echo "MODEL_DOWNLOAD_DIR: ${{env.MODEL_DOWNLOAD_DIR}}"    
        echo "repo_id: ${{ matrix.repo_id }}"   
        ./bin/llama-quantize $MODEL_DOWNLOAD_DIR/ibm-granite/granite-3.0-8b-instruct/f16.gguf quantized.gguf Q4_K_M

    # TODO:
    # - name: convert-lora-to-gguf
    #   run: |
    #     python ./llamacpp/convert_lora_to_gguf.py ...
