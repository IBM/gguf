# This workflow runs Build Verification Tests (BVT) on the quantized models
# using llama.cpp utilities.
name: lm-eval-quantized-models-gguf

on:
  workflow_dispatch:
  workflow_call:
    secrets:
      hf_token:
        required: true
    inputs:
      enable_language_jobs:
        type: boolean
        required: false
        default: false
      repo_id:
        type: string
        required: true
      quantization:
        type: string
        required: true
      target_repo_owner:
        type: string
        required: true
      target_repo_name_ext:
        type: string
        required: true
      ext_log:
        type: string
        required: false
        default: '.log.txt'
      ext_response:
        type: string
        required: false
        default: '.response.txt'
      debug:
        type: boolean
        required: true
        default: false
      trace:
        type: boolean
        required: false
        default: false

# See: https://github.com/ggerganov/llama.cpp/blob/master/examples/server/tests/unit/test_completion.py
env:
  DEBUG: ${{inputs.debug}}
  EXT_GGUF: .gguf
  LLAMA_SYSTEM_PROMPT: "You are a helpful assistant. Please ensure responses are professional, accurate, and safe."
  LLAMA_REQUEST_PROMPT: "Why is the sky blue according to science?"
  LLAMA_REQUEST_N_PREDICT: 128
  LLAMA_CLI_TOOL: llama-server
  LLAMA_SERVER_TEMP: 0.8
  LLAMA_SERVER_HOST: localhost
  LLAMA_SERVER_PORT: 8080
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models
  QUANTIZED_MODEL_EXISTS: false
  TEST_LLAMA_SERVER: true
  RUN_LM_EVAL: true
  GH_ARTIFACT_RETENTION: 2

jobs:
  test-quantized-model:
    runs-on: macos-latest
    if: ${{ inputs.enable_language_jobs == true }}
    env:
      HF_HUB_DISABLE_XET: 1
      HF_HUB_ENABLE_HF_TRANSFER: 0
      LLAMA_CPP_VERSION: "b6569" # 'b5717', # 'b6050'
    steps:
    - uses: actions/checkout@v4
    - name: confirm-environment
      run: |
        echo "[INFO] github.env:"
        echo "[INFO] >> run_id: '${{ github.run_id }}'"
        echo "[INFO] >> ref: '${{ github.ref }}', ref_name: '${{ github.ref_name }}', ref_type: '${{ github.ref_type }}'"
        echo "[INFO] >> workflow_ref: '${{ github.workflow_ref }}'"
        echo "[INFO] >> event_type: '${{ github.event_type }}'"
        echo "[INFO] >> event.: action: '${{ github.event.action }}'"
        echo "[INFO] >> event.: base_ref: '${{ github.event.base_ref }}'"
        echo "[INFO] >> event.: workflow_run.conclusion: '${{ github.event.workflow_run.conclusion }}'"
        echo "[INFO] >> event.release.: name: '${{ github.event.release.name }}', tag_name: '${{ github.event.release.tag_name }}'"

    - name: Dump GitHub context
      env:
        GH_CONTEXT: ${{ toJson(github) }}
      run: echo "$GH_CONTEXT"

    - name: Dump GitHub inputs
      env:
        GITHUB_INPUTS: ${{ toJson(inputs) }}
      run: echo "$GITHUB_INPUTS"

    # - name: List all environment variables
    #   if: ${{ github.event.inputs.debug }}
    #   run: env | sort

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    # Note: you can verify in a step using: run: python -c "import sys; print(sys.version)"
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # primarily huggingface_hub
    - name: install-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        pip list

    # Use this step to set values to the github context (shared across jobs/steps)
    # Note: using $GITHUB_OUTPUT sets values under the current step's namespace
    # whereas using $GITHUB_ENV sets values in the job's underlying environment.
    # Note: for each 'repo_id' we parse out e.g., REPO_ORG=ibm-granite REPO_NAME=granite-3.0-2b-instruct
    - name: set-github-env
      id: set_github_env
      run: |
        echo "REPO_ORG=$(dirname '${{ inputs.repo_id }}')" >> $GITHUB_ENV
        echo "REPO_NAME=$(basename '${{ inputs.repo_id }}')" >> $GITHUB_ENV

    - name: set-derivative-env-vars-1
      run: |
        echo "TARGET_REPO_ID=${{inputs.target_repo_owner}}/${{env.REPO_NAME}}${{inputs.target_repo_name_ext}}" >> $GITHUB_ENV
        echo "BASE_FNAME_QUANTIZED_GGUF=${{ env.REPO_NAME }}-${{inputs.quantization}}${{env.EXT_GGUF}}" >> $GITHUB_ENV
        echo "BASE_FNAME_LOG_FILE=${{env.REPO_NAME}}-${{inputs.quantization}}-${{env.LLAMA_CLI_TOOL}}${{inputs.ext_log}}" >> $GITHUB_ENV
        echo "BASE_FNAME_RESPONSE_FILE=${{env.REPO_NAME}}-${{inputs.quantization}}-${{env.LLAMA_CLI_TOOL}}${{inputs.ext_response}}" >> $GITHUB_ENV

    - name: set-derivative-env-vars-2
      run: |
        echo "LOCAL_MODEL_PATH=${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}" >> $GITHUB_ENV

    - name: set-derivative-env-vars-3
      run: |
        echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.LOCAL_MODEL_PATH}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}" >> $GITHUB_ENV

    - name: verify-github-env
      run: |
        echo "================== Derivative Environment Variables 1 =================="
        echo "TARGET_REPO_ID='$TARGET_REPO_ID' (${{ env.TARGET_REPO_ID }})"
        echo "BASE_FNAME_QUANTIZED_GGUF='$BASE_FNAME_QUANTIZED_GGUF' (${{ env.BASE_FNAME_QUANTIZED_GGUF }})"
        echo "BASE_FNAME_LOG_FILE='$BASE_FNAME_LOG_FILE' (${{ env.BASE_FNAME_LOG_FILE }})"
        echo "BASE_FNAME_RESPONSE_FILE='$BASE_FNAME_RESPONSE_FILE' (${{ env.BASE_FNAME_RESPONSE_FILE }})"
        echo "================== Derivative Environment Variables 2 =================="
        echo "LOCAL_MODEL_PATH='$LOCAL_MODEL_PATH' (${{ env.LOCAL_MODEL_PATH }})"
        echo "================== Derivative Environment Variables 3 =================="
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"

    - name: test-quantized-model-exists
      run: |
        exists=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.hf_token}})
        echo "exists: '$exists'"
        if [[ "$exists" == "False" ]]; then
          echo "FAILURE: model file: '${{env.TARGET_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' does not exist."
          exit 2
        else
          echo "SUCCESS: model file: '${{env.TARGET_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' exists."
          echo setting environment variable: QUANTIZED_MODEL_EXISTS='true'...
          echo "QUANTIZED_MODEL_EXISTS=true" >> $GITHUB_ENV
        fi

    - name: download-quantized-gguf-hf-hub-download
      if: env.QUANTIZED_MODEL_EXISTS == 'true'
      run: |
        echo "Downloading model to: ${{env.LOCAL_FNAME_QUANTIZED_GGUF}}..."
        echo "--------------------"
        python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.hf_token}}
        ls -al ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/*${{env.EXT_GGUF}}

    # - name: verify-downloaded-files
    #   if: env.DEBUG == 'true' && runner.os == 'macOS'
    #   run: |
    #     echo downloaded files...
    #     echo "--------------------"
    #     find . -name \*.gguf -type f
    #     echo "--------------------"

    # ========================= llama.cpp llama-server install ======================================

    # NOTE: pgrep: -o returns most recent process if multiple started; -o requires an exact name match
    - name: start-llama-server
      if: ${{ env.TEST_LLAMA_SERVER == 'true' || env.RUN_LM_EVAL == 'true' }}
      timeout-minutes: 10
      continue-on-error: true
      run: |
        echo "--------------------"
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"
        echo "--------------------"
        echo "Starting llama-server on port ${{env.LLAMA_SERVER_PORT}}"
        nohup ./bin/llama-server -m ./${{env.LOCAL_FNAME_QUANTIZED_GGUF}} --port ${{env.LLAMA_SERVER_PORT}} --temp ${{env.LLAMA_SERVER_TEMP}} --verbose --log-file ${{env.BASE_FNAME_LOG_FILE}} &
        echo "Llama server started in the background."
        echo waiting for llama-server to start...
        retry_count=0
        max_retries=10
        while ! nc -v -z ${{env.LLAMA_SERVER_HOST}} ${{env.LLAMA_SERVER_PORT}} && [ $retry_count -lt $max_retries ]; do
          echo "retry_count: $retry_count"
          if [ $? -eq 0 ]; then
            echo "Connection successful!"
            LLAMA_SERVER_PID=$(pgrep -o -x llama-server)
            echo "LLAMA_SERVER_PID: $LLAMA_SERVER_PID"
            echo "llama_server_pid=$LLAMA_SERVER_PID" >> $GITHUB_OUTPUT # Set the step output
            break
          else
            echo "Connection failed, retrying..."
            echo "sleeping 1 second..." && sleep 1
            retry_count=$((retry_count + 1))
          fi
        done

    - name: verify-llama-server-connection
      if: ${{ env.TEST_LLAMA_SERVER == 'true' || env.RUN_LM_EVAL == 'true' }}
      continue-on-error: true
      run: |
        echo "testing llama-server connection: ${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}..."
        echo "netstat:\n$(netstat -an -p tcp | grep ".${{env.LLAMA_SERVER_HOST}}")"

        if nc -v -z ${{env.LLAMA_SERVER_HOST}} ${{env.LLAMA_SERVER_PORT}}; then
          echo "SUCCESS: Connection to llama-server successful."
        else
          echo "Failure: Connection to llama-server failed."
          # ps aux
          exit 1
        fi

    - name: test-llama-server-with-curl
      timeout-minutes: 5
      if: ${{ env.TEST_LLAMA_SERVER == 'true' || env.RUN_LM_EVAL == 'true' }}
      continue-on-error: true
      run: |
        curl --request GET --url http://${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}/health
        echo "TODO: Test for {"status":"ok"} from GET /health"
        echo "${{env.LLAMA_REQUEST_PROMPT}}"
        curl --request POST --url http://${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}/completion --header "Content-Type: application/json" -o llama_response.json --data '{ "prompt": "${{env.LLAMA_REQUEST_PROMPT}}", "n_predict": ${{env.LLAMA_REQUEST_N_PREDICT}} }'
        echo "==="
        cat llama_response.json | jq .
        echo "==="

    # ========================= Install im-eval-harness ====================================

    - name: clone-install-lm-eval-harness
      if: env.RUN_LM_EVAL == 'true'
      timeout-minutes: 10
      continue-on-error: true
      run: |
        git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
        cd lm-evaluation-harness
        pip install -e .
        cd ..

    # ========================= Kill llama-server ===========================================

    # TODO: look to add confirmation of kill and take extra measures for gc if failed
    - name: kill-llama-server
      if: ${{ env.TEST_LLAMA_SERVER == 'true' || env.RUN_LM_EVAL == 'true' }}
      continue-on-error: true
      run: |
        echo "killing ${{env.LLAMA_CLI_TOOL}} ${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}} (PID: ${{ steps.start-llama-server.outputs.llama_server_pid }})..."
        pkill -9 ${{env.LLAMA_CLI_TOOL}}

    # ========================= Upload log as artifact ======================================

    - name: Upload log artifact
      if: env.TEST_LLAMA_SERVER == 'true'
      id: artifact-upload-llama-server-log
      uses: actions/upload-artifact@v4
      with:
        overwrite: true
        retention-days: ${{env.GH_ARTIFACT_RETENTION}}
        name: ${{env.REPO_NAME}}-${{inputs.quantization}}-${{env.LLAMA_CLI_TOOL}}-log
        path: ${{env.BASE_FNAME_LOG_FILE}}

    - name: Show log artifact ID
      if: env.TEST_LLAMA_SERVER == 'true'
      id: show-artifact-id-llama-server-log
      run:  echo 'Artifact ID is ${{ steps.artifact-upload-llama-server-log.outputs.artifact-id }}'

    # ========================= Upload response as artifact ======================================

    - name: Upload response artifact
      if: env.TEST_LLAMA_SERVER == 'true'
      id: artifact-upload-llama-server-response
      uses: actions/upload-artifact@v4
      with:
        overwrite: true
        retention-days: ${{env.GH_ARTIFACT_RETENTION}}
        name: ${{env.REPO_NAME}}-${{inputs.quantization}}-${{env.LLAMA_CLI_TOOL}}-response
        path: ${{env.BASE_FNAME_RESPONSE_FILE}}

    - name: Show response artifact ID
      if: env.TEST_LLAMA_SERVER == 'true'
      id: show-artifact-id-llama-server-response
      run:  echo 'Artifact ID is ${{ steps.artifact-upload-llama-server-response.outputs.artifact-id }}'
