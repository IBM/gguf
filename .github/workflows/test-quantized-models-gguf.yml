# This workflow generates quantized GGUF models from the original model (safetensors) from HF.
# For a given instance (i.e., HF model id (repo) and quantization):
# - Checks out (clones) the current source code repository
# - Sets environment variables used in the workflow
# - Sets up a Python 3.11 environment
# - Shallow clones the llama.cpp project (master branch)
# - Installs llama.cpp python conversion script dependencies 
# - downloads the designated model from (public) HF; primarily this are the *.safetensor files
# - Conversion (normalization): converts HF model (safetensors) to GGUF format (normalized to f16)
# - Quantization: converts the f16 GGUF to the designated quantization (e.g., Q4_0, Q4_K_M, etc.)
# - IN-PROGRESS: publish the result to a release (TODO, support private repo. and secret token)
# - TODO: push to an IBM HF repo. (collection) specifically for GGUF formatted Granite models

name: test-quantized-models-gguf

on:
  # Assure repos. have been created before any uploads (by ref. workflow)
  workflow_dispatch:       
 
permissions:
  contents: write
  packages: write

env:
  EXT_GGUF: .gguf
  FIND_ARGS: "-not -path '*/.*' -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models
  TARGET_REPO_NAME_EXT: -GGUF
  TARGET_REPO_OWNER: mrutkows # ibm-research
  DEBUG: true
  LLAMA_SERVER_PORT: 8080
  LLAMA_SERVER_LOCALHOST: "http://127.0.0.1"
  QUANTIZED_MODEL_EXISTS: false

# Full matrix (Ollama):
# repo_id: [
#   "ibm-granite/granite-3.0-2b-instruct", 
#   "ibm-granite/granite-3.0-8b-instruct",
#   "ibm-granite/granite-3.0-3b-a800m-instruct",
#   "ibm-granite/granite-3.0-1b-a400m-instruct",
#   "ibm-granite/granite-guardian-3.0-2b",
#   "ibm-granite/granite-guardian-3.0-8b", 
# ]
# quantization: [
#   "Q2_K", 
#   "Q3_K_L", "Q3_K_M", "Q3_K_S", 
#   "Q4_0", "Q4_1", "Q4_K_M", "Q4_K_S", 
#   "Q5_0", "Q5_1", "Q5_K_M", "Q5_K_S", 
#   "Q6_K", 
#   "Q8_0",
#]
jobs:
  build:
    runs-on: macos-latest
    strategy:
      fail-fast: false
      matrix:
        repo_id: [
          "ibm-granite/granite-3.0-1b-a400m-instruct",                  
        ]
        quantization: [
          "Q4_K_M", 
        ]

    steps:
    - uses: actions/checkout@v4

    - name: Dump runner context
      env:
        RUNNER_CONTEXT: ${{ toJson(runner) }}
      run: echo "$RUNNER_CONTEXT"

    - name: Dump GitHub context
      env:
        GITHUB_CONTEXT: ${{ toJson(github) }}
      run: echo "$GITHUB_CONTEXT"           
    
    # echo "$USER_ENV"  
    - name: confirm-environment
      run: |
        uname -a
        echo "runner.os: ${{ runner.os }}"
        echo "runner.arch: ${{ runner.arch }}"
        echo "github.workspace: ${{ github.workspace}}"
        echo "matrix.repo_id: ${{ matrix.repo_id }}"   
        echo "matrix.quantization: ${{ matrix.quantization }}"  
        echo "env.EXT_GGUF: ${{env.EXT_GGUF}}"      
        echo "env.FIND_ARGS: ${{env.FIND_ARGS}}"                           
        echo "env.LLAMACPP_DIR: ${{env.LLAMACPP_DIR}}"           
        echo "env.MODEL_DOWNLOAD_DIR: ${{env.MODEL_DOWNLOAD_DIR}}"        
        echo "env.TARGET_REPO_NAME_EXT: ${{env.TARGET_REPO_NAME_EXT}}"   
        echo "env.TARGET_REPO_OWNER: ${{env.TARGET_REPO_OWNER}}"          

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    # Note: you can verify in a step using: run: python -c "import sys; print(sys.version)"
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Use this step to set values to the github context (shared across jobs/steps)
    # Note: using $GITHUB_OUTPUT sets values under the current step's namespace
    # whereas using $GITHUB_ENV sets values in the job's underlying environment.
    # Note: for each 'repo_id' we parse out e.g., REPO_ORG=ibm-granite REPO_NAME=granite-3.0-2b-instruct
    - name: set-github-env
      id: set_github_env
      run: |       
        echo "REPO_ORG=$(dirname '${{ matrix.repo_id }}')" >> $GITHUB_ENV
        echo "REPO_NAME=$(basename '${{ matrix.repo_id }}')" >> $GITHUB_ENV
    
    - name: set-derivative-env-vars-1
      run: |   
        echo "TARGET_REPO_ID=${{env.TARGET_REPO_OWNER}}/${{env.REPO_NAME}}${{env.TARGET_REPO_NAME_EXT}}" >> $GITHUB_ENV         
        echo "BASE_FNAME_QUANTIZED_GGUF=${{ env.REPO_NAME }}-${{matrix.quantization}}${{env.EXT_GGUF}}" >> $GITHUB_ENV        

    - name: set-derivative-env-vars-2
      run: | 
        echo "LOCAL_MODEL_PATH=${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}" >> $GITHUB_ENV         
        echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.LOCAL_MODEL_PATH}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}" >> $GITHUB_ENV                 

    - name: verify-github-env
      run: |   
        echo "================== GITHUB_ENV =========================================="     
        echo "REPO_ORG=$REPO_ORG (${{ env.REPO_ORG }})"
        echo "REPO_NAME=$REPO_NAME (${{ env.REPO_NAME }})"  
        echo "================== Derivative Environment Variables 1 =================="           
        echo "BASE_FNAME_QUANTIZED_GGUF='$BASE_FNAME_QUANTIZED_GGUF' (${{ env.BASE_FNAME_QUANTIZED_GGUF }})" 
        echo "LOCAL_MODEL_PATH='$LOCAL_MODEL_PATH' (${{ env.LOCAL_MODEL_PATH }})"           
        echo "================== Derivative Environment Variables 2 =================="              
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"   
        echo "TARGET_REPO_ID='$TARGET_REPO_ID' (${{ env.TARGET_REPO_ID }})"   

    - name: install-hf-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        pip list   
  
    - name: test-quantized-model-exists
      run: |
        exists=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN}})   
        echo "exists: '$exists'"
        if [[ "$exists" == "False" ]]; then
          echo "ERROR: model file: '${{env.TARGET_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' does not exist."
          exit 2
        else 
          echo setting environment variable: QUANTIZED_MODEL_EXISTS='true'...
          echo "QUANTIZED_MODEL_EXISTS=true" >> $GITHUB_ENV
        fi

    - name: verify-file-to-download
      run: |       
        echo "--------------------"   
        find . -name \*.gguf -type f    
        echo "--------------------" 
        echo "QUANTIZED_MODEL_EXISTS: '${{env.QUANTIZED_MODEL_EXISTS}}'..."         
        echo "Downloading to: ${{env.LOCAL_FNAME_QUANTIZED_GGUF}}..." 
         
    - name: download-quantized-gguf-hf-hub-download
      if: env.QUANTIZED_MODEL_EXISTS == 'true'
      run: |    
        echo "Downloading using hf_file_download..."      
        echo "--------------------"
        python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN}}
        ls -al ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/*.gguf    

    - name: print-storage-memory-info
      if: env.DEBUG == 'true'    
      run: |
        df
        echo "--------------------"         
        xcodebuild -version        
        clang --version
        echo "--------------------"      
        top -l 1 | grep -E "^CPU|^PhysMem"  
        echo "--------------------"        
        
    - name: verify-downloaded-files
      if: env.DEBUG == 'true' 
      run: |
        echo downloaded files...    
        echo "--------------------"                       
        find . -name \*.gguf -type f    
        echo "--------------------"                   

    - name: start-llama-server-with-quantized-gguf
      run: |      
        echo "--------------------"             
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"          
        echo "--------------------"    
        echo "Staring llama-server on port ${{env.LLAMA_SERVER_PORT}}"      
        ./bin/llama-server -m ./${{env.LOCAL_FNAME_QUANTIZED_GGUF}} --port ${{env.LLAMA_SERVER_PORT}}     

    - name: test-inference-against-llama-server
      run: |
        llama-cli -s ${{env.LLAMA_SERVER_LOCALHOST}}:${{env.LLAMA_SERVER_PORT}} -i "What color is the sky?" -o response.txt
        cat response.txt
