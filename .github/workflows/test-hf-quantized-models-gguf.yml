# This workflow generates quantized GGUF models from the original model (safetensors) from HF.
# For a given instance (i.e., HF model id (repo) and quantization):
# - Checks out (clones) the current source code repository
# - Sets environment variables used in the workflow
# - Sets up a Python 3.11 environment
# - Shallow clones the llama.cpp project (master branch)
# - Installs llama.cpp python conversion script dependencies 
# - downloads the designated model from (public) HF; primarily this are the *.safetensor files
# - Conversion (normalization): converts HF model (safetensors) to GGUF format (normalized to f16)
# - Quantization: converts the f16 GGUF to the designated quantization (e.g., Q4_0, Q4_K_M, etc.)
# - IN-PROGRESS: publish the result to a release (TODO, support private repo. and secret token)
# - TODO: push to an IBM HF repo. (collection) specifically for GGUF formatted Granite models

name: g3.0-test-hf-quantized-models-gguf

on:
  # Assure repos. have been created before any uploads (by ref. workflow)
  workflow_dispatch:       
 
permissions:
  contents: write
  packages: write

env:
  DEBUG: true
  EXT_GGUF: .gguf
  FIND_ARGS: "-not -path '*/.*' -print | sed -e 's;[^/]*/;|___;g;s;___|; |;g'"
  LLAMA_CLI_RESPONSE_FILE: response.txt
  LLAMA_CLI_LOG_FILE: cli-log.txt
  LLAMA_CLI_SEED: 647063347
  LLAMA_SERVER_HOST: localhost
  LLAMA_SERVER_PORT: 8080
  LLAMACPP_DIR: llama.cpp
  MODEL_DOWNLOAD_DIR: models
  QUANTIZED_MODEL_EXISTS: false
  TARGET_REPO_NAME_EXT: -GGUF
  TARGET_REPO_OWNER: mrutkows # ibm-research
  TEST_LLAMA_SERVER: false
  TEST_LLAMA_CLI: true
  TEST_LLAMA_RUN: true

# Full matrix (Ollama):
# repo_id: [
#   "ibm-granite/granite-3.0-2b-instruct", 
#   "ibm-granite/granite-3.0-8b-instruct",
#   "ibm-granite/granite-3.0-3b-a800m-instruct",
#   "ibm-granite/granite-3.0-1b-a400m-instruct",
#   "ibm-granite/granite-guardian-3.0-2b",
#   "ibm-granite/granite-guardian-3.0-8b", 
# ]
# quantization: [
#   "Q2_K", 
#   "Q3_K_L", "Q3_K_M", "Q3_K_S", 
#   "Q4_0", "Q4_1", "Q4_K_M", "Q4_K_S", 
#   "Q5_0", "Q5_1", "Q5_K_M", "Q5_K_S", 
#   "Q6_K", 
#   "Q8_0",
#]
jobs:
  build:
    runs-on: macos-latest
    strategy:
      fail-fast: false
      matrix:
        repo_id: [
          "ibm-granite/granite-3.0-1b-a400m-instruct",                  
        ]
        quantization: [
          "Q4_K_M", 
        ]

    steps:
    - uses: actions/checkout@v4

    # - name: Dump runner context
    #   env:
    #     RUNNER_CONTEXT: ${{ toJson(runner) }}
    #   if: env.DEBUG == 'true'          
    #   run: echo "$RUNNER_CONTEXT"

    # - name: Dump GitHub context
    #   env:
    #     GITHUB_CONTEXT: ${{ toJson(github) }}
    #   if: env.DEBUG == 'true'          
    #   run: echo "$GITHUB_CONTEXT"           
    
    - name: confirm-environment
      run: |
        uname -a
        echo "runner.os: ${{ runner.os }}"
        echo "runner.arch: ${{ runner.arch }}"
        echo "github.workspace: ${{ github.workspace}}"
        echo "matrix.repo_id: ${{ matrix.repo_id }}"   
        echo "matrix.quantization: ${{ matrix.quantization }}"  
        echo "env.EXT_GGUF: ${{env.EXT_GGUF}}"      
        echo "env.FIND_ARGS: ${{env.FIND_ARGS}}"                           
        echo "env.LLAMACPP_DIR: ${{env.LLAMACPP_DIR}}"           
        echo "env.MODEL_DOWNLOAD_DIR: ${{env.MODEL_DOWNLOAD_DIR}}"        
        echo "env.TARGET_REPO_NAME_EXT: ${{env.TARGET_REPO_NAME_EXT}}"   
        echo "env.TARGET_REPO_OWNER: ${{env.TARGET_REPO_OWNER}}"          

    # Note: at the current time, we cannot use Python versions > 3.11 due to HF and langchain deps.
    # Note: you can verify in a step using: run: python -c "import sys; print(sys.version)"
    - uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Use this step to set values to the github context (shared across jobs/steps)
    # Note: using $GITHUB_OUTPUT sets values under the current step's namespace
    # whereas using $GITHUB_ENV sets values in the job's underlying environment.
    # Note: for each 'repo_id' we parse out e.g., REPO_ORG=ibm-granite REPO_NAME=granite-3.0-2b-instruct
    - name: set-github-env
      id: set_github_env
      run: |       
        echo "REPO_ORG=$(dirname '${{ matrix.repo_id }}')" >> $GITHUB_ENV
        echo "REPO_NAME=$(basename '${{ matrix.repo_id }}')" >> $GITHUB_ENV
    
    - name: set-derivative-env-vars-1
      run: |   
        echo "TARGET_REPO_ID=${{env.TARGET_REPO_OWNER}}/${{env.REPO_NAME}}${{env.TARGET_REPO_NAME_EXT}}" >> $GITHUB_ENV         
        echo "BASE_FNAME_QUANTIZED_GGUF=${{ env.REPO_NAME }}-${{matrix.quantization}}${{env.EXT_GGUF}}" >> $GITHUB_ENV        

    - name: set-derivative-env-vars-2
      run: | 
        echo "LOCAL_MODEL_PATH=${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}" >> $GITHUB_ENV         
    
    - name: set-derivative-env-vars-3
      run: |         
        echo "LOCAL_FNAME_QUANTIZED_GGUF=${{env.LOCAL_MODEL_PATH}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}" >> $GITHUB_ENV                 

    - name: verify-github-env
      run: |   
        echo "================== GITHUB_ENV =========================================="     
        echo "REPO_ORG=$REPO_ORG (${{ env.REPO_ORG }})"
        echo "REPO_NAME=$REPO_NAME (${{ env.REPO_NAME }})"  
        echo "================== Derivative Environment Variables 1 =================="    
        echo "TARGET_REPO_ID='$TARGET_REPO_ID' (${{ env.TARGET_REPO_ID }})"                 
        echo "BASE_FNAME_QUANTIZED_GGUF='$BASE_FNAME_QUANTIZED_GGUF' (${{ env.BASE_FNAME_QUANTIZED_GGUF }})" 
        echo "================== Derivative Environment Variables 2 ==================" 
        echo "LOCAL_MODEL_PATH='$LOCAL_MODEL_PATH' (${{ env.LOCAL_MODEL_PATH }})"           
        echo "================== Derivative Environment Variables 3 =================="              
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"    

    - name: install-hf-dependencies
      run: |
        python -m pip install -r ./requirements.txt
        pip list   
  
    - name: test-quantized-model-exists
      run: |
        exists=$(python ./scripts/hf_model_file_exists.py ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN}})   
        echo "exists: '$exists'"
        if [[ "$exists" == "False" ]]; then
          echo "FAILURE: model file: '${{env.TARGET_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' does not exist."
          exit 2
        else 
          echo "SUCCESS: model file: '${{env.TARGET_REPO_ID}}/${{env.BASE_FNAME_QUANTIZED_GGUF}}' exists."
          echo setting environment variable: QUANTIZED_MODEL_EXISTS='true'...
          echo "QUANTIZED_MODEL_EXISTS=true" >> $GITHUB_ENV
        fi        
         
    - name: download-quantized-gguf-hf-hub-download
      if: env.QUANTIZED_MODEL_EXISTS == 'true'
      run: |    
        echo "Downloading model to: ${{env.LOCAL_FNAME_QUANTIZED_GGUF}}..."      
        echo "--------------------"
        python ./scripts/hf_file_download.py ${{ env.MODEL_DOWNLOAD_DIR}} ${{ env.TARGET_REPO_ID }} ${{ env.BASE_FNAME_QUANTIZED_GGUF }} ${{secrets.HF_TOKEN}}
        ls -al ${{env.MODEL_DOWNLOAD_DIR}}/${{ env.TARGET_REPO_ID }}/*.gguf    

    - name: verify-downloaded-files
      if: env.DEBUG == 'true' 
      run: |
        echo downloaded files...    
        echo "--------------------"                       
        find . -name \*.gguf -type f    
        echo "--------------------" 
    
    - name: validate-sha256
      if: env.DEBUG == 'true' 
      run: |
        echo "validating sha256..."    
        echo "--------------------" 
        echo "TODO"
        # ./bin/llama-gguf-hash --sha256 ~/Downloads/granite-3.0-1b-a400m-instruct-Q4_K_M.gguf > sha256.txt

    - name: print-storage-memory-info
      if: env.DEBUG == 'true'    
      run: |
        df
        echo "--------------------"         
        xcodebuild -version        
        clang --version
        echo "--------------------"      
        top -l 1 | grep -E "^CPU|^PhysMem"  
        echo "--------------------"        
        
    - name: test-llama-server-with-quantized-gguf
      if: env.TEST_LLAMA_SERVER == 'true'     
      timeout-minutes: 5   
      run: |      
        echo "--------------------"             
        echo "LOCAL_FNAME_QUANTIZED_GGUF='$LOCAL_FNAME_QUANTIZED_GGUF' (${{ env.LOCAL_FNAME_QUANTIZED_GGUF }})"          
        echo "--------------------"    
        echo "Starting llama-server on port ${{env.LLAMA_SERVER_PORT}}"      
        nohup ./bin/llama-server -m ./${{env.LOCAL_FNAME_QUANTIZED_GGUF}} --port ${{env.LLAMA_SERVER_PORT}} &         

    # echo waiting for llama-server to start...  
    # retry_count=0
    # max_retries=10
    # while ! nc -v -z localhost ${{env.LLAMA_SERVER_PORT}} && [ $retry_count -lt $max_retries ]; do
    #   echo "retry_count: $retry_count"
    #   if [ $? -eq 0 ]; then
    #     echo "Connection successful!"
    #     break
    #   else
    #     echo "Connection failed, retrying..."
    #     echo "sleeping 1 second..." && sleep 1
    #     retry_count=$((retry_count + 1)) 
    #   fi          
    # done          
    - name: test-llama-server-connection
      if: env.TEST_LLAMA_SERVER == 'true'     
      run: |
        echo "testing llama-server connection: ${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}..."
        echo "netstat:\n$(netstat -an -p tcp | grep ".${{env.LLAMA_SERVER_HOST}}")"

        if nc -v -z ${{env.LLAMA_SERVER_HOST}} ${{env.LLAMA_SERVER_PORT}}; then
          echo "SUCCESS: Connection to llama-server successful."
        else
          echo "Failure: Connection to llama-server failed."
          # ps aux
          exit 1
        fi           

    - name: test-llama-server-with-curl
      timeout-minutes: 5
      if: env.TEST_LLAMA_SERVER == 'true'  
      run: | 
        curl --request GET --url http://${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}/health      
        echo "TODO: Test for {"status":"ok"} from GET /health"
        curl --request POST \
          --url ttp://${{env.LLAMA_SERVER_HOST}}:${{env.LLAMA_SERVER_PORT}}/completion \
          --header "Content-Type: application/json" \
          --data '{"prompt": "The sky is blue because:","n_predict": 128, "seed": ${{env.LLAMA_CLI_SEED}}}'
        echo "TODO: Test for JSON response from POST /completion endpoint using jq"          
    
    - name: kill-llama-server
      if: env.TEST_LLAMA_SERVER == 'true'  
      run: |
        echo "TODO: is netstat and ps commands to get PID and kill..." 

    - name: test-llama-cli
      timeout-minutes: 5
      if: env.TEST_LLAMA_CLI == 'true'  
      run: |  
        echo "invoking llama-cli to test inference (generation)..."
        ./bin/llama-cli \
          -s ${{env.LLAMA_CLI_SEED}} \
          -m ./${{env.LOCAL_FNAME_QUANTIZED_GGUF}} \
          -p "What color is the sky?" \
          -no-cnv \
          -n 128 \
          --log-file ${{env.LLAMA_CLI_LOG_FILE}} \
          2>/dev/null \
          1>${{env.LLAMA_CLI_RESPONSE_FILE}}  
        if [ $? -eq 1 ]; then
          exit 1
        fi  

    - name: test-print-llama-cli-response-with-metrics
      if: env.TEST_LLAMA_CLI == 'true'      
      run: |
        echo "llama-cli: ${{env.LLAMA_CLI_RESPONSE_FILE}} ($(wc -w ${{env.LLAMA_CLI_RESPONSE_FILE}} | awk '{print $1}') words):\n'$(cat ${{env.LLAMA_CLI_RESPONSE_FILE}})'"

    - name: test-print-llama-cli-generated-log
      if: env.TEST_LLAMA_CLI == 'true' && env.DEBUG == 'true'      
      run: |
        echo "llama-cli: ${{env.LLAMA_CLI_LOG_FILE}}: '$(cat ${{env.LLAMA_CLI_LOG_FILE}})'"     

    - name: test-llama-run
      if: env.TEST_LLAMA_RUN == 'true'
      run: |
        echo "invoking llama-run to test inference..."      
        ./bin/llama-run ./${{env.LOCAL_FNAME_QUANTIZED_GGUF}} -p "What color is the sky?"
        if [ $? -eq 1 ]; then
          exit 1
        fi  
